# ingestor-scrapper

Un proyecto de Scrapy con Clean Architecture (Ports & Adapters) para aprender web scraping desde cero, pero con una estructura escalable y ordenada desde el inicio.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un scaffold mÃ­nimo pero funcional de Scrapy siguiendo los principios de Clean Architecture (Hexagonal Architecture). La estructura estÃ¡ diseÃ±ada para crecer sin necesidad de reestructurar todo el proyecto cuando se agreguen nuevas funcionalidades.

## ğŸ—ï¸ Arquitectura

El proyecto estÃ¡ organizado en capas siguiendo Clean Architecture:

```
ingestor_scrapper/
â”œâ”€ core/                    # Dominio (framework-agnÃ³stico)
â”‚  â”œâ”€ entities.py          # Modelos del dominio (Item, Page, Document, Record, ContentType)
â”‚  â””â”€ ports.py             # Interfaces (HtmlFetcher, DocumentFetcher, HtmlParser, 
â”‚                           #            TabularParser, PdfParser, Normalizer, OutputPort)
â”‚
â”œâ”€ application/            # Casos de uso (orquestan puertos)
â”‚  â”œâ”€ use_cases.py         # Casos de uso base y genÃ©ricos
â”‚  â”œâ”€ bcra_use_case.py      # BcraUseCase
â”‚  â”œâ”€ bcra_monetario_use_case.py  # BcraMonetarioUseCase
â”‚  â”œâ”€ parser_router.py     # ParserRouter (selecciona parser por ContentType)
â”‚  â”œâ”€ universal_ingest_use_case.py  # UniversalIngestUseCase (mÃºltiples formatos)
â”‚  â””â”€ tests/               # Tests de use cases
â”‚
â”œâ”€ adapters/               # Implementaciones (dependientes de frameworks)
â”‚  â”œâ”€ fetchers/
â”‚  â”‚  â”œâ”€ http.py          # AdapterScrapyFetcher, AdapterScrapyDocumentFetcher
â”‚  â”‚  â””â”€ scrapy.py        # AdapterHttpFetcher (stub para requests)
â”‚  â”œâ”€ parsers/
â”‚  â”‚  â”œâ”€ bs4.py           # AdapterBs4Parser (stub, requiere beautifulsoup4)
â”‚  â”‚  â”œâ”€ bcra.py          # AdapterBcraParser (funciona)
â”‚  â”‚  â”œâ”€ bcra_excel.py    # AdapterBcraExcelParser (Excel BCRA, funciona)
â”‚  â”‚  â”œâ”€ csv.py           # AdapterCsvParser (stub bÃ¡sico)
â”‚  â”‚  â”œâ”€ excel.py         # AdapterExcelParser (stub, requiere openpyxl)
â”‚  â”‚  â”œâ”€ pdf.py           # AdapterPdfParser (stub, requiere pdfplumber)
â”‚  â”‚  â”œâ”€ registry.py      # PARSER_REGISTRY (registro centralizado)
â”‚  â”‚  â””â”€ tests/           # Tests de parsers
â”‚  â”œâ”€ normalizers/
â”‚  â”‚  â”œâ”€ bcra.py          # AdapterBcraNormalizer
â”‚  â”‚  â”œâ”€ bcra_monetario.py  # AdapterBcraMonetarioNormalizer (funciona)
â”‚  â”‚  â”œâ”€ generic.py       # AdapterGenericNormalizer (fallback)
â”‚  â”‚  â””â”€ tests/           # Tests de normalizers
â”‚  â””â”€ outputs/
â”‚     â”œâ”€ stdout.py        # AdapterStdoutOutput
â”‚     â””â”€ json.py          # AdapterJsonOutput
â”‚
â””â”€ interface/              # Entrada/Delivery (spiders, CLI)
   â””â”€ spiders/
      â”œâ”€ bcra_spider.py      # Spider para BCRA HTML (funciona)
      â”œâ”€ bcra_monetario_spider.py  # Spider BCRA Excel (funciona)
      â””â”€ universal_spider.py  # Spider genÃ©rico con ParserRouter (ejemplo)
```

### PatrÃ³n Puertos y Adaptadores

- **Puertos (Ports)**: Interfaces/Protocolos definidos en `core/ports.py` que representan contratos abstractos.
- **Adaptadores (Adapters)**: Implementaciones concretas en `adapters/` que implementan esos puertos usando frameworks especÃ­ficos (Scrapy, BeautifulSoup, etc.).

Esto permite que la lÃ³gica de negocio (`application/`) permanezca independiente de frameworks externos.

### Soporte para MÃºltiples Formatos

El proyecto ahora soporta mÃºltiples formatos de documentos:
- **HTML**: Parsing con BeautifulSoup4 (stub, requiere instalaciÃ³n)
- **CSV**: Parsing con mÃ³dulo `csv` nativo (stub bÃ¡sico)
- **Excel (XLS/XLSX)**: Parsing con openpyxl/xlrd (stub, requiere instalaciÃ³n)
- **PDF**: Parsing con pdfplumber/tabula-py (stub, requiere instalaciÃ³n)

El **ParserRouter** selecciona automÃ¡ticamente el parser correcto segÃºn el Content-Type del documento.

## ğŸ“š DocumentaciÃ³n

- ğŸ“– [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md) - GuÃ­a completa de la arquitectura y cÃ³mo agregar nuevos sitios/formatos
- ğŸ•·ï¸ [CÃ³mo Funciona Scrapy](docs/COMO_SCRAPY_FUNCIONA.md) - ExplicaciÃ³n de cÃ³mo Scrapy pasa el response al spider
- ğŸ” [CÃ³mo Scrapy Busca Variables](docs/COMO_SCRAPY_BUSCA_VARIABLES.md) - CÃ³mo Scrapy encuentra y usa las variables del spider
- ğŸ¯ [Para QuÃ© Sirve el Normalizer](docs/PARA_QUE_SIRVE_NORMALIZER.md) - ExplicaciÃ³n del rol del Normalizer en la arquitectura

## ğŸš€ InstalaciÃ³n

### 1. Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate
```

### 2. Instalar dependencias

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Uso

### Ejecutar spiders

```bash
# Spider BCRA (HTML)
make crawl SPIDER=bcra
# o directamente:
scrapy crawl bcra

# Spider BCRA Monetario (Excel)
make crawl SPIDER=bcra_monetario
# o directamente:
scrapy crawl bcra_monetario

# Spider universal (mÃºltiples formatos)
scrapy crawl universal -a url="https://example.com"
```

**Archivos de salida:**
- `bcra_data.json` - Datos de BCRA Principales Variables (HTML)
- `bcra_monetario_data.json` - Datos de BCRA Informe Monetario Diario (Excel)

### Ejecutar tests

```bash
# Todos los tests
make test

# Tests con cobertura
make test-cov

# O directamente con pytest
pytest ingestor_scrapper/ -v
```

**Cobertura actual:** 87% de los mÃ³dulos principales.

### Ejemplo: Spider BCRA Monetario

El spider `bcra_monetario` muestra el patrÃ³n completo de Clean Architecture:

```python
from ingestor_scrapper.adapters.fetchers import AdapterScrapyDocumentFetcher
from ingestor_scrapper.adapters.parsers.bcra_excel import AdapterBcraExcelParser
from ingestor_scrapper.adapters.normalizers.bcra_monetario import AdapterBcraMonetarioNormalizer
from ingestor_scrapper.adapters.outputs import AdapterJsonOutput
from ingestor_scrapper.application.bcra_monetario_use_case import BcraMonetarioUseCase

class BcraMonetarioSpider(scrapy.Spider):
    name = "bcra_monetario"
    start_urls = ["https://www.bcra.gob.ar/..."]

    def parse_excel(self, response):
        fetcher = AdapterScrapyDocumentFetcher(response)
        parser = AdapterBcraExcelParser()
        normalizer = AdapterBcraMonetarioNormalizer()
        output = AdapterJsonOutput(output_file="bcra_monetario_data.json")
        
        use_case = BcraMonetarioUseCase(fetcher, parser, normalizer, output)
        items = use_case.execute(response.url)
```

### Agregar nuevos spiders

Para crear un nuevo spider, consulta: [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md)

## ğŸ“¦ Estructura del Proyecto

- **`core/`**: Capa de dominio con entidades y puertos (interfaces). Framework-agnÃ³stico.
- **`application/`**: Casos de uso que orquestan los puertos. Incluye tests.
- **`adapters/`**: Implementaciones concretas de los puertos. Incluye tests organizados por mÃ³dulo.
- **`interface/`**: Puntos de entrada (spiders de Scrapy, futuros CLI, APIs, etc.).

## ğŸ—ºï¸ Roadmap

### PrÃ³ximos pasos sugeridos:

1. **Implementar parsers de stubs**: Completar implementaciÃ³n de parsers para CSV, PDF
   - CSV: EstÃ¡ bÃ¡sico, expandir funcionalidad
   - PDF: Instalar pdfplumber y completar parser

2. **Pipelines de Scrapy**: Activar pipelines para procesamiento de items
   - Descomentar secciÃ³n de pipelines en `settings.py`
   - Crear pipelines para validaciÃ³n, limpieza, almacenamiento

3. **Storage**: Agregar adaptadores de salida a archivos/base de datos
   - `AdapterDatabaseOutput` para persistir en DB
   - `AdapterApiOutput` para enviar a APIs

4. **Expandir tests**: Aumentar cobertura
   - Tests de fetchers, outputs
   - Tests de integraciÃ³n end-to-end

## ğŸ“ Notas

- El proyecto sigue el patrÃ³n de **parser por proveedor/sitio** para mÃ¡xima flexibilidad.
- Los tests estÃ¡n organizados junto a los mÃ³dulos que testean.
- Para scrapear un nuevo sitio, consulta [Arquitectura Escalable](docs/ARQUITECTURA_SCALABLE.md).
- Todos los archivos incluyen **TODOs** donde se puede expandir la funcionalidad.

## ğŸ§ª Testing

El proyecto incluye tests unitarios con 87% de cobertura:

```bash
# Ejecutar todos los tests
make test

# Ejecutar con cobertura detallada
make test-cov

# Linting automÃ¡tico
ruff check --fix ingestor_scrapper/ tests/
ruff format ingestor_scrapper/ tests/
```

Tests organizados por mÃ³dulo:
- `adapters/parsers/tests/` - Tests de parsers (13 tests)
- `adapters/normalizers/tests/` - Tests de normalizers (9 tests)  
- `application/tests/` - Tests de use cases (8 tests)

## ğŸ“š Referencias

- [Scrapy Documentation](https://docs.scrapy.org/)
- [Clean Architecture (Hexagonal Architecture)](https://alistair.cockburn.us/hexagonal-architecture/)
- [Ports & Adapters Pattern](https://herbertograca.com/2017/11/16/explicit-architecture-01-ddd-hexagonal-onion-clean-cqrs-how-i-put-it-all-together/)
